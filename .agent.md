# zenjiro-bot-oci Development Guide

## Project Overview

This is a collection of Twitter bots designed to monitor and report various service statuses and emergency information in Japan. The bots run on Oracle Cloud Infrastructure (OCI) and are written primarily in Bash with Python utilities for Twitter API interaction.

### Purpose
- Monitor service outages, transportation delays, and emergency situations
- Automatically tweet status updates when changes are detected
- Provide real-time information to Japanese users about various services

### Languages and Technologies
- **Primary Language**: Bash scripting
- **Supporting Language**: Python 3
- **Platform**: Oracle Cloud Infrastructure (OCI)
- **APIs**: Twitter API v2
- **Dependencies**: `nkf`, `jq`, `curl`, `requests`, `requests-oauthlib`

## Bot Categories

### Transportation Bots
- **nanbulinebot**: JR East Nanbu Line status monitoring
- **tokyulinebot**: Tokyu railway line status monitoring

### Emergency/Fire Department Bots
- **kawasakifirebot**: Kawasaki city fire department incidents
- **kawasakiwarnbot**: Kawasaki city warnings and alerts
- **ykhmfirebot**: Yokohama city fire department incidents

### Service Outage Bots
- **iijmiotrblbot**: IIJmio mobile service trouble reports
- **wimaxoutagebot**: UQ WiMAX service outage monitoring
- **ocnmobileonebot**: OCN Mobile ONE service monitoring

## Key Files and Structure

### Core Scripts
- `run-all.sh` - Main orchestration script that runs all bots
- `tweet.sh` - Comprehensive Twitter API client (from TomoTool project)
- `post.py` - Python Twitter API posting utility
- `truncate.py` - Text truncation utility for Twitter character limits

### Bot Scripts
- `*bot.sh` - Individual bot scripts (pattern: `*{b,B}ot.sh`)
- Each bot follows a consistent pattern of data fetching, processing, and formatting

### Configuration
- `environment` - Contains Twitter API credentials for all bots
- `requirements.txt` - Python dependencies

## Development Conventions

### Bot Script Patterns
1. **Status Indicators**: All bots use emoji status indicators:
   - üî¥ (red) - Critical issues, outages, emergencies
   - üü† (orange) - Warnings, delays, partial issues
   - üü¢ (green) - Normal operation, resolved issues
   - ‚ö™ (white) - Neutral/informational status

2. **Script Structure**:
   ```bash
   #!/bin/bash
   set -u
   # Define emoji variables
   red=üî¥
   orange=üü†
   green=üü¢
   # Fetch and process data with curl/sed/awk
   ```

3. **Data Processing Pipeline**:
   - Fetch data with `curl`
   - Process HTML/JSON with `sed`, `awk`, `grep`, `jq`
   - Apply text transformations with `nkf` (character encoding)
   - Add status emoji based on content patterns

### Text Processing Conventions
- Use `nkf` for Japanese character encoding normalization
- Convert full-width numbers to half-width: `y/ÔºêÔºëÔºíÔºìÔºîÔºïÔºñÔºóÔºòÔºô/0123456789/`
- Remove HTML tags: `s/<[^>]+>//g`
- Clean whitespace and formatting consistently
- Truncate output to fit Twitter's character limits

### Environment Variables
- Bot credentials follow pattern: `{botname}_API_KEY`, `{botname}_ACCESS_TOKEN`, etc.
- All credentials stored in `environment` file
- Use lowercase bot names for environment variable prefixes

## Best Practices

### Bot Development
1. **Error Handling**: Use `set -u` to catch undefined variables
2. **Pipefail**: Use `set -o pipefail` for pipeline error detection
3. **Fallback Messages**: Provide default "normal operation" messages when data unavailable
4. **Consistent Formatting**: Follow established emoji and text formatting patterns

### Data Fetching
1. **Robust Parsing**: Use multiple sed/awk commands for reliable HTML parsing
2. **Error Recovery**: Handle network failures gracefully
3. **Rate Limiting**: Respect source website rate limits
4. **Caching**: Use diff comparison to avoid duplicate tweets

### Security
1. **Credential Management**: Keep API keys in environment file, not in scripts
2. **Input Sanitization**: Sanitize data before posting to Twitter
3. **Log Sanitization**: Remove credentials from debug logs

## Deployment and Operations

### Installation Requirements
```bash
sudo apt install nkf jq python3 python3-pip
pip3 install -r requirements.txt
```

### Running the Bots
```bash
export $(xargs < environment) && ./run-all.sh
```

### Bot Execution Flow
1. `run-all.sh` iterates through all `*bot.sh` files
2. Sets up environment variables for each bot
3. Runs bot script and pipes output through `truncate.py`
4. Compares output with previous run using `diff`
5. Posts to Twitter only if content changed
6. Uses either `post.py` (for specific bots) or `tweet.sh` (for others)

### File Management
- `.{botname}-now` - Current bot output
- `.{botname}-last` - Previous bot output for comparison
- Temporary files are managed automatically by `run-all.sh`

## Troubleshooting

### Common Issues
1. **Character Encoding**: Ensure `nkf` is properly handling Japanese text
2. **API Rate Limits**: Monitor Twitter API usage
3. **Website Changes**: Bot scripts may break if source websites change structure
4. **Network Connectivity**: Ensure reliable internet connection for data fetching

### Debugging
- Set `DEBUG=1` environment variable for detailed `tweet.sh` logging
- Check individual bot outputs by running scripts manually
- Verify API credentials are correctly set in environment file

## Contributing

### Issues and Pull Requests
- Use the `gh` command for creating and managing issues and pull requests
- Examples:
  ```bash
  # Create a new issue
  gh issue create --title "Bug: Bot not posting tweets" --body "Description of the issue"
  
  # Create a pull request
  gh pr create --title "Fix: Update bot parsing logic" --body "Description of changes"
  
  # List issues
  gh issue list
  
  # View pull requests
  gh pr list
  ```

### Adding New Bots
1. Create new `{name}bot.sh` script following established patterns
2. Add Twitter API credentials to `environment` file
3. Implement data fetching and processing logic
4. Use consistent emoji status indicators
5. Test thoroughly before deployment

### Modifying Existing Bots
1. Understand the data source and parsing logic
2. Maintain backward compatibility where possible
3. Test changes against current data sources
4. Update documentation if behavior changes significantly